From 36d9466bf80e9a0f24eb184efbd8a2c9100289db Mon Sep 17 00:00:00 2001
From: John Fitzpatrick <john@cyberfitz.org>
Date: Wed, 13 Aug 2025 21:59:28 -0700
Subject: [PATCH 23/38] fix: Resolve source_id inconsistency and FastAPI
 routing issues
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

- Fix source_id extraction to consistently handle .git suffixes across all services
- Add Base64 encoding for DELETE endpoints to handle special characters in source_ids
- Update extract_source_id_from_url functions in all storage and crawling services
- Prevent foreign key constraint violations in code examples table
- Enable proper deletion of knowledge items with complex source_ids

ðŸ¤– Generated with [Claude Code](https://claude.ai/code)

Co-Authored-By: Claude <noreply@anthropic.com>
---
 .../src/services/knowledgeBaseService.ts      |  4 +-
 python/src/server/api_routes/knowledge_api.py | 48 ++++++++++++++-----
 .../crawling/code_extraction_service.py       | 19 ++++++--
 .../services/crawling/crawling_service.py     | 17 +++++--
 .../crawling/document_storage_operations.py   |  4 +-
 .../services/storage/code_storage_service.py  | 17 +++++--
 .../storage/document_storage_service.py       | 17 +++++--
 7 files changed, 97 insertions(+), 29 deletions(-)

diff --git a/archon-ui-main/src/services/knowledgeBaseService.ts b/archon-ui-main/src/services/knowledgeBaseService.ts
index 2a672e9..3469d61 100644
--- a/archon-ui-main/src/services/knowledgeBaseService.ts
+++ b/archon-ui-main/src/services/knowledgeBaseService.ts
@@ -179,7 +179,9 @@ class KnowledgeBaseService {
    * Delete a knowledge item by source_id
    */
   async deleteKnowledgeItem(sourceId: string) {
-    return apiRequest(`/knowledge-items/${sourceId}`, {
+    // Base64 encode the source_id to handle special characters like forward slashes
+    const encodedSourceId = btoa(sourceId)
+    return apiRequest(`/knowledge-items/${encodedSourceId}`, {
       method: 'DELETE'
     })
   }
diff --git a/python/src/server/api_routes/knowledge_api.py b/python/src/server/api_routes/knowledge_api.py
index 6907310..2fd631c 100644
--- a/python/src/server/api_routes/knowledge_api.py
+++ b/python/src/server/api_routes/knowledge_api.py
@@ -180,8 +180,17 @@ async def update_knowledge_item(source_id: str, updates: dict):
 async def delete_knowledge_item(source_id: str):
     """Delete a knowledge item from the database."""
     try:
-        logger.debug(f"Starting delete_knowledge_item for source_id: {source_id}")
-        safe_logfire_info(f"Deleting knowledge item | source_id={source_id}")
+        # Decode Base64 encoded source_id to handle special characters
+        import base64
+        try:
+            decoded_source_id = base64.b64decode(source_id.encode()).decode()
+            logger.debug(f"Decoded source_id from {source_id} to {decoded_source_id}")
+        except Exception as decode_error:
+            logger.warning(f"Failed to decode source_id {source_id}, using as-is: {decode_error}")
+            decoded_source_id = source_id
+        
+        logger.debug(f"Starting delete_knowledge_item for source_id: {decoded_source_id}")
+        safe_logfire_info(f"Deleting knowledge item | source_id={decoded_source_id}")
 
         # Use SourceManagementService directly instead of going through MCP
         logger.debug("Creating SourceManagementService...")
@@ -191,7 +200,7 @@ async def delete_knowledge_item(source_id: str):
         logger.debug("Successfully created SourceManagementService")
 
         logger.debug("Calling delete_source function...")
-        success, result_data = source_service.delete_source(source_id)
+        success, result_data = source_service.delete_source(decoded_source_id)
         logger.debug(f"delete_source returned: success={success}, data={result_data}")
 
         # Convert to expected format
@@ -202,25 +211,27 @@ async def delete_knowledge_item(source_id: str):
         }
 
         if result.get("success"):
-            safe_logfire_info(f"Knowledge item deleted successfully | source_id={source_id}")
+            safe_logfire_info(f"Knowledge item deleted successfully | source_id={decoded_source_id}")
 
-            return {"success": True, "message": f"Successfully deleted knowledge item {source_id}"}
+            return {"success": True, "message": f"Successfully deleted knowledge item {decoded_source_id}"}
         else:
             safe_logfire_error(
-                f"Knowledge item deletion failed | source_id={source_id} | error={result.get('error')}"
+                f"Knowledge item deletion failed | source_id={decoded_source_id} | error={result.get('error')}"
             )
             raise HTTPException(
                 status_code=500, detail={"error": result.get("error", "Deletion failed")}
             )
 
     except Exception as e:
+        # Use decoded_source_id if available, otherwise fallback to source_id
+        error_source_id = locals().get('decoded_source_id', source_id)
         logger.error(f"Exception in delete_knowledge_item: {e}")
         logger.error(f"Exception type: {type(e)}")
         import traceback
 
         logger.error(f"Traceback: {traceback.format_exc()}")
         safe_logfire_error(
-            f"Failed to delete knowledge item | error={str(e)} | source_id={source_id}"
+            f"Failed to delete knowledge item | error={str(e)} | source_id={error_source_id}"
         )
         raise HTTPException(status_code=500, detail={"error": str(e)})
 
@@ -817,26 +828,35 @@ async def get_available_sources():
 async def delete_source(source_id: str):
     """Delete a source and all its associated data."""
     try:
-        safe_logfire_info(f"Deleting source | source_id={source_id}")
+        # Decode Base64 encoded source_id to handle special characters
+        import base64
+        try:
+            decoded_source_id = base64.b64decode(source_id.encode()).decode()
+            logger.debug(f"Decoded source_id from {source_id} to {decoded_source_id}")
+        except Exception as decode_error:
+            logger.warning(f"Failed to decode source_id {source_id}, using as-is: {decode_error}")
+            decoded_source_id = source_id
+        
+        safe_logfire_info(f"Deleting source | source_id={decoded_source_id}")
 
         # Use SourceManagementService directly
         from ..services.source_management_service import SourceManagementService
 
         source_service = SourceManagementService(get_supabase_client())
 
-        success, result_data = source_service.delete_source(source_id)
+        success, result_data = source_service.delete_source(decoded_source_id)
 
         if success:
-            safe_logfire_info(f"Source deleted successfully | source_id={source_id}")
+            safe_logfire_info(f"Source deleted successfully | source_id={decoded_source_id}")
 
             return {
                 "success": True,
-                "message": f"Successfully deleted source {source_id}",
+                "message": f"Successfully deleted source {decoded_source_id}",
                 **result_data,
             }
         else:
             safe_logfire_error(
-                f"Source deletion failed | source_id={source_id} | error={result_data.get('error')}"
+                f"Source deletion failed | source_id={decoded_source_id} | error={result_data.get('error')}"
             )
             raise HTTPException(
                 status_code=500, detail={"error": result_data.get("error", "Deletion failed")}
@@ -844,7 +864,9 @@ async def delete_source(source_id: str):
     except HTTPException:
         raise
     except Exception as e:
-        safe_logfire_error(f"Failed to delete source | error={str(e)} | source_id={source_id}")
+        # Use decoded_source_id if available, otherwise fallback to source_id
+        error_source_id = locals().get('decoded_source_id', source_id)
+        safe_logfire_error(f"Failed to delete source | error={str(e)} | source_id={error_source_id}")
         raise HTTPException(status_code=500, detail={"error": str(e)})
 
 
diff --git a/python/src/server/services/crawling/code_extraction_service.py b/python/src/server/services/crawling/code_extraction_service.py
index 5a8b1e8..dcda728 100644
--- a/python/src/server/services/crawling/code_extraction_service.py
+++ b/python/src/server/services/crawling/code_extraction_service.py
@@ -21,7 +21,7 @@ def extract_source_id_from_url(url: str) -> str:
     """
     Extract a meaningful source_id from a URL.
     
-    For GitHub URLs, includes the repository path (e.g., github.com/user/repo).
+    For GitHub URLs, includes the repository path (e.g., github.com/user/repo or github.com/user/repo.git).
     For other URLs, uses the domain.
     
     Args:
@@ -35,11 +35,22 @@ def extract_source_id_from_url(url: str) -> str:
     
     # Special handling for GitHub URLs to include repository path
     if domain == "github.com" and parsed_url.path:
-        # Extract user/repo from path like /user/repo/blob/main/file.py
+        # Extract user/repo from path like /user/repo/blob/main/file.py or /user/repo.git
         path_parts = [part for part in parsed_url.path.split('/') if part]
         if len(path_parts) >= 2:
-            # Include github.com/user/repo
-            return f"github.com/{path_parts[0]}/{path_parts[1]}"
+            # Check if the second part ends with .git
+            repo_part = path_parts[1]
+            if repo_part.endswith('.git'):
+                # Include github.com/user/repo.git (preserve .git suffix)
+                return f"github.com/{path_parts[0]}/{repo_part}"
+            else:
+                # Check if this might be from a git clone URL by looking for .git in original URL
+                if '.git' in url:
+                    # Add .git suffix to match source records created during crawl
+                    return f"github.com/{path_parts[0]}/{repo_part}.git"
+                else:
+                    # Include github.com/user/repo (no .git suffix)
+                    return f"github.com/{path_parts[0]}/{repo_part}"
     
     return domain
 
diff --git a/python/src/server/services/crawling/crawling_service.py b/python/src/server/services/crawling/crawling_service.py
index 4a724f2..af34a37 100644
--- a/python/src/server/services/crawling/crawling_service.py
+++ b/python/src/server/services/crawling/crawling_service.py
@@ -19,7 +19,7 @@ def extract_source_id_from_url(url: str) -> str:
     """
     Extract a meaningful source_id from a URL.
     
-    For GitHub URLs, includes the repository path (e.g., github.com/user/repo).
+    For GitHub URLs, includes the repository path (e.g., github.com/user/repo.git).
     For other URLs, uses the domain.
     
     Args:
@@ -36,8 +36,19 @@ def extract_source_id_from_url(url: str) -> str:
         # Extract user/repo from path like /user/repo/blob/main/file.py
         path_parts = [part for part in parsed_url.path.split('/') if part]
         if len(path_parts) >= 2:
-            # Include github.com/user/repo
-            return f"github.com/{path_parts[0]}/{path_parts[1]}"
+            # Check if the second part ends with .git
+            repo_part = path_parts[1]
+            if repo_part.endswith('.git'):
+                # Include github.com/user/repo.git (preserve .git suffix)
+                return f"github.com/{path_parts[0]}/{repo_part}"
+            else:
+                # Check if this might be from a git clone URL by looking for .git in original URL
+                if '.git' in url:
+                    # Add .git suffix to match source records created during crawl
+                    return f"github.com/{path_parts[0]}/{repo_part}.git"
+                else:
+                    # Standard GitHub URL without .git
+                    return f"github.com/{path_parts[0]}/{repo_part}"
     
     return domain
 
diff --git a/python/src/server/services/crawling/document_storage_operations.py b/python/src/server/services/crawling/document_storage_operations.py
index 90624a2..b4ad80a 100644
--- a/python/src/server/services/crawling/document_storage_operations.py
+++ b/python/src/server/services/crawling/document_storage_operations.py
@@ -214,7 +214,7 @@ class DocumentStorageOperations:
             
             # Generate summary with fallback
             try:
-                summary = extract_source_summary(source_id, combined_content)
+                summary = await extract_source_summary(source_id, combined_content)
             except Exception as e:
                 safe_logfire_error(f"Failed to generate AI summary for '{source_id}': {str(e)}, using fallback")
                 # Fallback to simple summary
@@ -223,7 +223,7 @@ class DocumentStorageOperations:
             # Update source info in database BEFORE storing documents
             safe_logfire_info(f"About to create/update source record for '{source_id}' (word count: {source_id_word_counts[source_id]})")
             try:
-                update_source_info(
+                await update_source_info(
                     client=self.supabase_client,
                     source_id=source_id,
                     summary=summary,
diff --git a/python/src/server/services/storage/code_storage_service.py b/python/src/server/services/storage/code_storage_service.py
index 946bfe3..e1b5e8e 100644
--- a/python/src/server/services/storage/code_storage_service.py
+++ b/python/src/server/services/storage/code_storage_service.py
@@ -25,7 +25,7 @@ def extract_source_id_from_url(url: str) -> str:
     """
     Extract a meaningful source_id from a URL.
     
-    For GitHub URLs, includes the repository path (e.g., github.com/user/repo).
+    For GitHub URLs, includes the repository path (e.g., github.com/user/repo.git).
     For other URLs, uses the domain.
     
     Args:
@@ -42,8 +42,19 @@ def extract_source_id_from_url(url: str) -> str:
         # Extract user/repo from path like /user/repo/blob/main/file.py
         path_parts = [part for part in parsed_url.path.split('/') if part]
         if len(path_parts) >= 2:
-            # Include github.com/user/repo
-            return f"github.com/{path_parts[0]}/{path_parts[1]}"
+            # Check if the second part ends with .git
+            repo_part = path_parts[1]
+            if repo_part.endswith('.git'):
+                # Include github.com/user/repo.git (preserve .git suffix)
+                return f"github.com/{path_parts[0]}/{repo_part}"
+            else:
+                # Check if this might be from a git clone URL by looking for .git in original URL
+                if '.git' in url:
+                    # Add .git suffix to match source records created during crawl
+                    return f"github.com/{path_parts[0]}/{repo_part}.git"
+                else:
+                    # Standard GitHub URL without .git
+                    return f"github.com/{path_parts[0]}/{repo_part}"
     
     return domain
 
diff --git a/python/src/server/services/storage/document_storage_service.py b/python/src/server/services/storage/document_storage_service.py
index 4f70bde..f740fda 100644
--- a/python/src/server/services/storage/document_storage_service.py
+++ b/python/src/server/services/storage/document_storage_service.py
@@ -23,7 +23,7 @@ def extract_source_id_from_url(url: str) -> str:
     """
     Extract a meaningful source_id from a URL.
     
-    For GitHub URLs, includes the repository path (e.g., github.com/user/repo).
+    For GitHub URLs, includes the repository path (e.g., github.com/user/repo.git).
     For other URLs, uses the domain.
     
     Args:
@@ -40,8 +40,19 @@ def extract_source_id_from_url(url: str) -> str:
         # Extract user/repo from path like /user/repo/blob/main/file.py
         path_parts = [part for part in parsed_url.path.split('/') if part]
         if len(path_parts) >= 2:
-            # Include github.com/user/repo
-            return f"github.com/{path_parts[0]}/{path_parts[1]}"
+            # Check if the second part ends with .git
+            repo_part = path_parts[1]
+            if repo_part.endswith('.git'):
+                # Include github.com/user/repo.git (preserve .git suffix)
+                return f"github.com/{path_parts[0]}/{repo_part}"
+            else:
+                # Check if this might be from a git clone URL by looking for .git in original URL
+                if '.git' in url:
+                    # Add .git suffix to match source records created during crawl
+                    return f"github.com/{path_parts[0]}/{repo_part}.git"
+                else:
+                    # Standard GitHub URL without .git
+                    return f"github.com/{path_parts[0]}/{repo_part}"
     
     return domain
 
-- 
2.39.5

