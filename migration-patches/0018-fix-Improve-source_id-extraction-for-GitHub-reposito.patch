From 0fc583414787dd04966995fd198c2ffb3dcc351d Mon Sep 17 00:00:00 2001
From: John Fitzpatrick <john@cyberfitz.org>
Date: Wed, 13 Aug 2025 16:08:00 -0700
Subject: [PATCH 18/38] fix: Improve source_id extraction for GitHub
 repositories
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

- Add extract_source_id_from_url() helper function across storage services
- GitHub URLs now generate unique source_ids like github.com/user/repo
- Previously all GitHub repos were grouped under single github.com source_id
- Fixes Knowledge Base showing single card instead of separate repo cards
- Applied to document_storage, code_storage, code_extraction, and crawling services

ðŸ¤– Generated with [Claude Code](https://claude.ai/code)

Co-Authored-By: Claude <noreply@anthropic.com>
---
 .../crawling/code_extraction_service.py       |  30 +++-
 .../services/crawling/crawling_service.py     |  31 +++-
 .../services/storage/code_storage_service.py  | 141 +++++++++---------
 .../storage/document_storage_service.py       |  30 +++-
 4 files changed, 152 insertions(+), 80 deletions(-)

diff --git a/python/src/server/services/crawling/code_extraction_service.py b/python/src/server/services/crawling/code_extraction_service.py
index 71e12eb..5a8b1e8 100644
--- a/python/src/server/services/crawling/code_extraction_service.py
+++ b/python/src/server/services/crawling/code_extraction_service.py
@@ -17,6 +17,33 @@ from ..storage.code_storage_service import (
 )
 
 
+def extract_source_id_from_url(url: str) -> str:
+    """
+    Extract a meaningful source_id from a URL.
+    
+    For GitHub URLs, includes the repository path (e.g., github.com/user/repo).
+    For other URLs, uses the domain.
+    
+    Args:
+        url: The URL to extract source_id from
+        
+    Returns:
+        str: The extracted source_id
+    """
+    parsed_url = urlparse(url)
+    domain = parsed_url.netloc or parsed_url.path
+    
+    # Special handling for GitHub URLs to include repository path
+    if domain == "github.com" and parsed_url.path:
+        # Extract user/repo from path like /user/repo/blob/main/file.py
+        path_parts = [part for part in parsed_url.path.split('/') if part]
+        if len(path_parts) >= 2:
+            # Include github.com/user/repo
+            return f"github.com/{path_parts[0]}/{path_parts[1]}"
+    
+    return domain
+
+
 class CodeExtractionService:
     """
     Service for extracting and processing code examples from documents.
@@ -307,8 +334,7 @@ class CodeExtractionService:
 
                 if code_blocks:
                     # Always extract source_id from URL
-                    parsed_url = urlparse(source_url)
-                    source_id = parsed_url.netloc or parsed_url.path
+                    source_id = extract_source_id_from_url(source_url)
 
                     for block in code_blocks:
                         all_code_blocks.append({
diff --git a/python/src/server/services/crawling/crawling_service.py b/python/src/server/services/crawling/crawling_service.py
index b450375..4a724f2 100644
--- a/python/src/server/services/crawling/crawling_service.py
+++ b/python/src/server/services/crawling/crawling_service.py
@@ -14,6 +14,34 @@ from urllib.parse import urlparse
 from ...config.logfire_config import safe_logfire_info, safe_logfire_error, get_logger
 from ...utils import get_supabase_client
 
+
+def extract_source_id_from_url(url: str) -> str:
+    """
+    Extract a meaningful source_id from a URL.
+    
+    For GitHub URLs, includes the repository path (e.g., github.com/user/repo).
+    For other URLs, uses the domain.
+    
+    Args:
+        url: The URL to extract source_id from
+        
+    Returns:
+        str: The extracted source_id
+    """
+    parsed_url = urlparse(url)
+    domain = parsed_url.netloc or parsed_url.path
+    
+    # Special handling for GitHub URLs to include repository path
+    if domain == "github.com" and parsed_url.path:
+        # Extract user/repo from path like /user/repo/blob/main/file.py
+        path_parts = [part for part in parsed_url.path.split('/') if part]
+        if len(path_parts) >= 2:
+            # Include github.com/user/repo
+            return f"github.com/{path_parts[0]}/{path_parts[1]}"
+    
+    return domain
+
+
 # Lazy import socket.IO handlers to avoid circular dependencies
 # These are imported as module-level variables but resolved at runtime
 update_crawl_progress = None
@@ -279,8 +307,7 @@ class CrawlingService:
             safe_logfire_info(f"Starting async crawl orchestration | url={url} | task_id={task_id}")
             
             # Extract source_id from the original URL
-            parsed_original_url = urlparse(url)
-            original_source_id = parsed_original_url.netloc or parsed_original_url.path
+            original_source_id = extract_source_id_from_url(url)
             safe_logfire_info(f"Using source_id '{original_source_id}' from original URL '{url}'")
             
             # Helper to update progress with mapper
diff --git a/python/src/server/services/storage/code_storage_service.py b/python/src/server/services/storage/code_storage_service.py
index 7eeaf14..946bfe3 100644
--- a/python/src/server/services/storage/code_storage_service.py
+++ b/python/src/server/services/storage/code_storage_service.py
@@ -18,6 +18,34 @@ from supabase import Client
 from ...config.logfire_config import search_logger
 from ..embeddings.embedding_service import create_embeddings_batch, create_embedding, get_dimension_column_name
 from ..embeddings.contextual_embedding_service import generate_contextual_embeddings_batch
+from ..llm_provider_service import get_llm_client
+
+
+def extract_source_id_from_url(url: str) -> str:
+    """
+    Extract a meaningful source_id from a URL.
+    
+    For GitHub URLs, includes the repository path (e.g., github.com/user/repo).
+    For other URLs, uses the domain.
+    
+    Args:
+        url: The URL to extract source_id from
+        
+    Returns:
+        str: The extracted source_id
+    """
+    parsed_url = urlparse(url)
+    domain = parsed_url.netloc or parsed_url.path
+    
+    # Special handling for GitHub URLs to include repository path
+    if domain == "github.com" and parsed_url.path:
+        # Extract user/repo from path like /user/repo/blob/main/file.py
+        path_parts = [part for part in parsed_url.path.split('/') if part]
+        if len(path_parts) >= 2:
+            # Include github.com/user/repo
+            return f"github.com/{path_parts[0]}/{path_parts[1]}"
+    
+    return domain
 
 
 def _get_model_choice() -> str:
@@ -30,6 +58,9 @@ def _get_model_choice() -> str:
             model = credential_service._cache["MODEL_CHOICE"]
         else:
             model = os.getenv("MODEL_CHOICE", "gpt-4.1-nano")
+        
+        # Strip whitespace to fix issues with trailing spaces in model names
+        model = model.strip() if model else "gpt-4.1-nano"
         search_logger.debug(f"Using model choice: {model}")
         return model
     except Exception as e:
@@ -489,7 +520,7 @@ def extract_code_blocks(markdown_content: str, min_length: int = None) -> list[d
     return grouped_blocks
 
 
-def generate_code_example_summary(
+async def generate_code_example_summary(
     code: str, context_before: str, context_after: str, language: str = "", provider: str = None
 ) -> dict[str, str]:
     """
@@ -535,82 +566,48 @@ Format your response as JSON:
 """
 
     try:
-        # Get LLM client using fallback
-        try:
-            import os
-
-            import openai
-
-            api_key = os.getenv("OPENAI_API_KEY")
-            if not api_key:
-                # Try to get from credential service with direct fallback
-                from ..credential_service import credential_service
-
-                if (
-                    credential_service._cache_initialized
-                    and "OPENAI_API_KEY" in credential_service._cache
-                ):
-                    cached_key = credential_service._cache["OPENAI_API_KEY"]
-                    if isinstance(cached_key, dict) and cached_key.get("is_encrypted"):
-                        api_key = credential_service._decrypt_value(cached_key["encrypted_value"])
-                    else:
-                        api_key = cached_key
-                else:
-                    api_key = os.getenv("OPENAI_API_KEY", "")
-
-            if not api_key:
-                raise ValueError("No OpenAI API key available")
-
-            client = openai.OpenAI(api_key=api_key)
-        except Exception as e:
-            search_logger.error(
-                f"Failed to create LLM client fallback: {e} - returning default values"
+        # Use the LLM provider service instead of hardcoded OpenAI
+        async with get_llm_client(provider=provider) as client:
+            search_logger.debug(
+                f"Calling LLM API with model: {model_choice}, language: {language}, code length: {len(code)}"
             )
-            return {
-                "example_name": f"Code Example{f' ({language})' if language else ''}",
-                "summary": "Code example for demonstration purposes.",
-            }
-
-        search_logger.debug(
-            f"Calling OpenAI API with model: {model_choice}, language: {language}, code length: {len(code)}"
-        )
 
-        response = client.chat.completions.create(
-            model=model_choice,
-            messages=[
-                {
-                    "role": "system",
-                    "content": "You are a helpful assistant that analyzes code examples and provides JSON responses with example names and summaries.",
-                },
-                {"role": "user", "content": prompt},
-            ],
-            response_format={"type": "json_object"},
-        )
+            response = await client.chat.completions.create(
+                model=model_choice,
+                messages=[
+                    {
+                        "role": "system",
+                        "content": "You are a helpful assistant that analyzes code examples and provides JSON responses with example names and summaries.",
+                    },
+                    {"role": "user", "content": prompt},
+                ],
+                response_format={"type": "json_object"},
+            )
 
-        response_content = response.choices[0].message.content.strip()
-        search_logger.debug(f"OpenAI API response: {repr(response_content[:200])}...")
+            response_content = response.choices[0].message.content.strip()
+            search_logger.debug(f"LLM API response: {repr(response_content[:200])}...")
 
-        result = json.loads(response_content)
+            result = json.loads(response_content)
 
-        # Validate the response has the required fields
-        if not result.get("example_name") or not result.get("summary"):
-            search_logger.warning(f"Incomplete response from OpenAI: {result}")
+            # Validate the response has the required fields
+            if not result.get("example_name") or not result.get("summary"):
+                search_logger.warning(f"Incomplete response from LLM: {result}")
 
-        final_result = {
-            "example_name": result.get(
-                "example_name", f"Code Example{f' ({language})' if language else ''}"
-            ),
-            "summary": result.get("summary", "Code example for demonstration purposes."),
-        }
+            final_result = {
+                "example_name": result.get(
+                    "example_name", f"Code Example{f' ({language})' if language else ''}"
+                ),
+                "summary": result.get("summary", "Code example for demonstration purposes."),
+            }
 
-        search_logger.info(
-            f"Generated code example summary - Name: '{final_result['example_name']}', Summary length: {len(final_result['summary'])}"
-        )
-        return final_result
+            search_logger.info(
+                f"Generated code example summary - Name: '{final_result['example_name']}', Summary length: {len(final_result['summary'])}"
+            )
+            return final_result
 
     except json.JSONDecodeError as e:
         search_logger.error(
-            f"Failed to parse JSON response from OpenAI: {e}, Response: {repr(response_content) if 'response_content' in locals() else 'No response'}"
+            f"Failed to parse JSON response from LLM: {e}, Response: {repr(response_content) if 'response_content' in locals() else 'No response'}"
         )
         return {
             "example_name": f"Code Example{f' ({language})' if language else ''}",
@@ -671,11 +668,8 @@ async def generate_code_summaries_batch(
             # Add delay between requests to avoid rate limiting
             await asyncio.sleep(0.5)  # 500ms delay between requests
 
-            # Run the synchronous function in a thread
-            loop = asyncio.get_event_loop()
-            result = await loop.run_in_executor(
-                None,
-                generate_code_example_summary,
+            # Call the now-async function directly
+            result = await generate_code_example_summary(
                 block["code"],
                 block["context_before"],
                 block["context_after"],
@@ -890,8 +884,7 @@ async def add_code_examples_to_supabase(
             if metadatas[idx] and "source_id" in metadatas[idx]:
                 source_id = metadatas[idx]["source_id"]
             else:
-                parsed_url = urlparse(urls[idx])
-                source_id = parsed_url.netloc or parsed_url.path
+                source_id = extract_source_id_from_url(urls[idx])
             
             # Get appropriate embedding column name based on dimensions
             try:
diff --git a/python/src/server/services/storage/document_storage_service.py b/python/src/server/services/storage/document_storage_service.py
index 7b31fb9..4f70bde 100644
--- a/python/src/server/services/storage/document_storage_service.py
+++ b/python/src/server/services/storage/document_storage_service.py
@@ -19,6 +19,33 @@ from ..embeddings.dimension_validator import (
 from ..embeddings.exceptions import VectorStorageError, DimensionValidationError
 
 
+def extract_source_id_from_url(url: str) -> str:
+    """
+    Extract a meaningful source_id from a URL.
+    
+    For GitHub URLs, includes the repository path (e.g., github.com/user/repo).
+    For other URLs, uses the domain.
+    
+    Args:
+        url: The URL to extract source_id from
+        
+    Returns:
+        str: The extracted source_id
+    """
+    parsed_url = urlparse(url)
+    domain = parsed_url.netloc or parsed_url.path
+    
+    # Special handling for GitHub URLs to include repository path
+    if domain == "github.com" and parsed_url.path:
+        # Extract user/repo from path like /user/repo/blob/main/file.py
+        path_parts = [part for part in parsed_url.path.split('/') if part]
+        if len(path_parts) >= 2:
+            # Include github.com/user/repo
+            return f"github.com/{path_parts[0]}/{path_parts[1]}"
+    
+    return domain
+
+
 async def add_documents_to_supabase(
     client,
     urls: list[str],
@@ -298,8 +325,7 @@ async def add_documents_to_supabase(
                     source_id = batch_metadatas[j]["source_id"]
                 else:
                     # Fallback: Extract source_id from URL
-                    parsed_url = urlparse(batch_urls[j])
-                    source_id = parsed_url.netloc or parsed_url.path
+                    source_id = extract_source_id_from_url(batch_urls[j])
                 
                 # Get appropriate embedding column name based on dimensions
                 try:
-- 
2.39.5

